# NicheFinder: Product Requirements Document

**Version:** 2.0
**Date:** 2025-12-11
**Status:** Active Development
**Owner:** JG

---

## Executive Summary

**NicheFinder** is a multi-domain market intelligence platform that demonstrates the end-to-end capabilities of the UDM + PEG + Connector ecosystem by systematically analyzing integration ecosystems to identify high-value opportunities using real-time API data.

**Primary Use Case:** Home Assistant Integration Opportunity Finder
**Architecture:** Extensible multi-niche framework supporting future domains

### Primary Goals

1. **Prove Connector Generation**: Demonstrate auto-generation of PEG-Connector v2 configurations from real-world APIs (GitHub, Reddit, HACS)
2. **Prove UDM Normalization**: Show data unification across heterogeneous sources into canonical schemas
3. **Prove PEG Orchestration**: Demonstrate workflow automation using built-in PEG executor
4. **Deliver Business Value**: Create actionable market intelligence that complements AI-based research
5. **Demonstrate Extensibility**: Build multi-niche architecture that can analyze any integration ecosystem

---

## Problem Statement

### Business Problem

**For Integration Developers:**
Identifying high-value integration opportunities requires manual research across fragmented data sources (GitHub issues, community forums, Reddit discussions, existing integrations). This process is time-consuming, inconsistent, and often misses emerging opportunities.

**For Market Intelligence:**
AI tools like ChatGPT provide qualitative insights but lack:
- Real-time, current data (training cutoff limitations)
- Verifiable sources and quantitative metrics
- Systematic, repeatable analysis
- Automated monitoring and trend tracking
- Multi-source data fusion

### Technical Problem

We need to prove that the UDM + PEG + Connector ecosystem can:
- Auto-generate connectors from any API without manual coding
- Normalize data from multiple heterogeneous sources into unified models
- Orchestrate complex multi-step workflows reliably
- Deliver quantitative, verifiable, real-time market intelligence
- Provide structured data that complements AI analysis

---

## Solution Overview

### Primary Use Case: Home Assistant Integration Opportunities

NicheFinder analyzes the Home Assistant ecosystem to identify the top integration opportunities by:

1. **Data Collection**: Fetch real-time data via auto-generated connectors:
   - GitHub API: Integration requests, issues, repository metrics
   - Reddit API: Community discussions, feature requests
   - HACS API: Existing custom integrations, download counts
   - Home Assistant Community Forum: Pain points, discussions

2. **Normalization**: Use UDM to unify heterogeneous data into canonical `IntegrationOpportunity` schema

3. **Analysis**: Calculate opportunity scores based on:
   - Demand signals (GitHub issues, Reddit posts, forum threads)
   - Supply analysis (existing integrations, quality, maintenance status)
   - Market size estimation (user mentions, engagement metrics)
   - Trend analysis (growth over time)

4. **Scoring**: Rank opportunities using demand-supply gap algorithm

5. **Output**: Generate actionable reports with:
   - Quantitative scores and metrics
   - Verifiable source links
   - Revenue potential estimates
   - Trend analysis
   - Optional AI-enhanced insights

### Multi-Niche Architecture

The system is designed to support multiple analysis domains:

**Current:**
- ‚úÖ Home Assistant Integrations (primary use case)

**Future Expansion:**
- üîÑ Developer Tools & Libraries (GitHub + Stack Overflow + npm)
- üîÑ SaaS Integration Marketplace (Zapier, Make, n8n analysis)
- üîÑ Content & Creator Niches (Reddit + News API + Hacker News)
- üîÑ Custom domains (user-defined configurations)

---

## Value Proposition

### How NicheFinder Complements AI Search

| Capability | ChatGPT/AI Tools | NicheFinder | Hybrid Approach |
|------------|------------------|-------------|-----------------|
| **Data Freshness** | ‚ùå Training cutoff (months old) | ‚úÖ Real-time API data | ‚úÖ Current data + AI reasoning |
| **Verifiable Sources** | ‚ùå Can't cite specific sources | ‚úÖ Links to GitHub issues, Reddit posts | ‚úÖ Verified data + context |
| **Quantitative Metrics** | ‚ö†Ô∏è Qualitative estimates | ‚úÖ Precise scores, counts, trends | ‚úÖ Metrics + interpretation |
| **Repeatability** | ‚ùå Different answers each time | ‚úÖ Consistent, trackable results | ‚úÖ Consistent + adaptive |
| **Automated Monitoring** | ‚ùå Manual re-querying needed | ‚úÖ Scheduled analysis, alerts | ‚úÖ Auto-update + summaries |
| **Systematic Coverage** | ‚ö†Ô∏è May miss opportunities | ‚úÖ Checks ALL sources | ‚úÖ Complete + prioritized |
| **Trend Analysis** | ‚ùå No historical tracking | ‚úÖ Growth rates, time-series | ‚úÖ Trends + predictions |
| **Multi-Source Fusion** | ‚ùå Can't query APIs directly | ‚úÖ Combines GitHub + Reddit + Forums | ‚úÖ Integrated + synthesized |

### Key Differentiators

**1. Real-Time Market Intelligence**
- ChatGPT: "Based on my training data from 2023, Govee integrations were popular..."
- NicheFinder: "In the last 30 days: 47 new GitHub issues, 234 Reddit posts, +45% growth"

**2. Verifiable, Quantitative Analysis**
- ChatGPT: "I think there's demand for better Govee support"
- NicheFinder: "Opportunity Score: 94/100 | Demand:Supply Ratio: 15:1 | Sources: [links]"

**3. Continuous Monitoring**
- ChatGPT: Requires manual re-querying
- NicheFinder: "Run every Monday, alert on new opportunities scoring >85"

**4. Structured Data for AI**
- NicheFinder can feed its structured, verified data to AI for enhanced insights
- Best of both worlds: quantitative data + qualitative reasoning

### Target Users

**Primary:**
- Integration developers seeking profitable opportunities
- Home Assistant power users identifying gaps
- Market researchers analyzing integration ecosystems

**Secondary:**
- Product managers validating integration roadmaps
- Investors evaluating integration platform opportunities
- Community maintainers prioritizing feature requests

---

## Architecture

### Design Principle: Use UDM-Single As-Is

**Critical:** We do NOT modify UDM-Single code. We use it as a dependency and build our business logic on top.

**What UDM-Single Provides (Use As-Is):**
- ‚úÖ Connector generation (`udm-connector-generator`)
- ‚úÖ Connector execution with retry, timeout, cache, circuit breaker (`udm-connectors`)
- ‚úÖ PEG workflow orchestration (`udm-peg`)
- ‚úÖ Concurrent execution limits
- ‚úÖ Built-in PEG executor (no external service needed)

**What We Contribute to UDM-Single:**
- üîÑ API rate limit tracking (benefits all UDM-Single users)

**What We Build:**
- ‚ùå Opportunity scoring algorithm (demand-supply gap)
- ‚ùå Report generation
- ‚ùå REST API server (Axum)
- ‚ùå Job scheduler (cron-based, scheduled analysis)
- ‚ùå Web UI (React + TypeScript)
- ‚ùå CLI interface (one-off analysis)

**See:** `UDM_CAPABILITIES_ANALYSIS.md` for detailed breakdown

---

### System Components

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    NicheFinder Multi-Domain CLI                 ‚îÇ
‚îÇ                        (This Repository)                        ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Niche Config ‚îÇ  ‚îÇ CLI Interface‚îÇ  ‚îÇ Report Generator     ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (HA, DevTools‚îÇ  ‚îÇ (Multi-niche)‚îÇ  ‚îÇ (Quantitative + AI)  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ  SaaS, etc.) ‚îÇ  ‚îÇ              ‚îÇ  ‚îÇ                      ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Dependency: UDM-Single                       ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ Connector        ‚îÇ  ‚îÇ UDM Core         ‚îÇ  ‚îÇ PEG Executor ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ Generator        ‚îÇ  ‚îÇ (Normalization)  ‚îÇ  ‚îÇ (Built-in)   ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                                 ‚îÇ
                                 ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                      External APIs (Free Tier)                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
‚îÇ  ‚îÇ GitHub API   ‚îÇ  ‚îÇ Reddit API   ‚îÇ  ‚îÇ HACS/Community APIs  ‚îÇ  ‚îÇ
‚îÇ  ‚îÇ (5K req/hr)  ‚îÇ  ‚îÇ (60 req/min) ‚îÇ  ‚îÇ (Unlimited)          ‚îÇ  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Component Breakdown (Lean Architecture):**

| Component | Type | Description |
|-----------|------|-------------|
| **Web UI** | üì¶ NEW (React + TS) | Single-page app for niche selection, results, scheduling |
| **nichefinder-server** | üì¶ NEW CRATE | Axum REST API + scheduler + SQLite (single binary) |
| **nichefinder-core** | üì¶ NEW CRATE | Scoring algorithm, types, report formatters (library) |
| **Niche Config** | üìÑ YAML FILES | Per-niche configuration (APIs, schemas, scoring) |
| **PEG Workflows** | üìÑ YAML FILES | Workflow definitions (e.g., home-assistant-analysis.yaml) |
| **UDM Core** | ‚úÖ USE AS-IS | Data normalization from UDM-Single |
| **PEG Executor** | ‚úÖ USE AS-IS | Workflow orchestration from UDM-Single |
| **Connectors** | ‚úÖ USE AS-IS | HTTP connector execution from UDM-Single |
| **Rate Limiter** | üîÑ CONTRIBUTE | Add to UDM-Single's udm-connectors crate |

**Legend:**
- ‚úÖ **USE AS-IS** - UDM-Single crates, no modifications
- üì¶ **NEW CRATE** - We build this in `crates/` (only 2 crates!)
- üì¶ **NEW (React + TS)** - React + TypeScript frontend (minimal dependencies)
- üìÑ **YAML FILES** - Configuration/workflow files
- üîÑ **CONTRIBUTE** - Implement in UDM-Single and contribute back

**Key Simplifications:**
- **2 Rust crates** instead of 6 (consolidated)
- **No separate CLI** - use REST API directly (`curl` for scripting)
- **Scheduler integrated** into server (not a separate crate)
- **Minimal frontend** - no routing library, no chart library for MVP

---

## User Interface

### Overview

NicheFinder provides a **web-based UI** built with React and TypeScript, running locally on the user's machine. The UI communicates with a Rust-based REST API server that orchestrates analysis workflows and manages scheduled jobs.

**Deployment Model:**
- **MVP:** Local-only (runs on localhost:3001)
- **Future:** Self-hosted or hosted service

---

### Architecture

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    Web Browser (localhost:3000)             ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ         React + TypeScript Frontend                  ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Niche      ‚îÇ  ‚îÇ Results    ‚îÇ  ‚îÇ Schedule     ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îÇ Selector   ‚îÇ  ‚îÇ Table      ‚îÇ  ‚îÇ Manager      ‚îÇ   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº HTTP REST API
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              Rust API Server (localhost:3001)               ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Axum REST API                                        ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ GET  /api/niches          - List niches          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ POST /api/analyze         - Run analysis         ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ GET  /api/opportunities   - Get results          ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ GET  /api/schedules       - List schedules       ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ POST /api/schedules       - Create schedule      ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ GET  /api/runs            - Job run history      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ Job Scheduler (tokio-cron-scheduler)                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Cron-based scheduling                             ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Execute PEG workflows on schedule                 ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ Store results in SQLite                           ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                          ‚îÇ
                          ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                    PEG Workflow Executor                    ‚îÇ
‚îÇ  (UDM-Single's built-in executor)                           ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

### Core Features (MVP)

#### 1. Niche Selection
- Dropdown to select analysis domain (Home Assistant for MVP)
- Display niche description and data sources

#### 2. Results View
- **Table View:** Sortable list of opportunities
  - Columns: Name, Demand Score, Supply Score, Gap Score, Sources
  - Click row to expand details inline
- **Detail View:** Expanded opportunity information
  - Demand signals (GitHub issues, Reddit posts)
  - Supply analysis (existing integrations, HACS availability)
  - Source links (verifiable, clickable)

#### 3. Scheduled Analysis (Core Feature)
- **Create Schedule:**
  - Select niche
  - Set schedule (daily, weekly)
  - Enable/disable schedule
- **View Schedules:**
  - List all scheduled jobs
  - Show next run time
  - Delete schedules
- **Run History:**
  - View past analysis runs
  - View results from each run

#### 4. Export
- Export current results as JSON
- Download as Markdown report

### Future Features (NOT MVP)
- ~~Trend charts~~ ‚Üí Requires multiple runs over time
- ~~Historical comparison~~ ‚Üí Requires weeks of data
- ~~Email notifications~~ ‚Üí Requires email service
- ~~Multiple niches~~ ‚Üí Start with Home Assistant only

---

### Tech Stack (Minimal)

**Frontend (Simplified for MVP):**
- **Framework:** React 18 + TypeScript
- **Build Tool:** Vite
- **Styling:** Tailwind CSS
- **HTTP Client:** Native `fetch` (no Axios needed)

**What we're NOT using for MVP:**
- ~~React Query~~ ‚Üí Simple `useState` + `useEffect`
- ~~React Router~~ ‚Üí Single page app, no routing needed
- ~~Recharts~~ ‚Üí Charts are future enhancement
- ~~Axios~~ ‚Üí Native `fetch` is sufficient

**Backend (Rust):**
- **Framework:** Axum (Rust async web framework)
- **Database:** SQLite (for schedules, results, job history)
- **Scheduler:** tokio-cron-scheduler (integrated into server)
- **ORM:** SQLx (async SQL toolkit)

---

### Data Persistence

**SQLite Schema:**

```sql
-- Scheduled jobs
CREATE TABLE schedules (
    id TEXT PRIMARY KEY,
    niche TEXT NOT NULL,
    cron_expression TEXT NOT NULL,
    enabled BOOLEAN NOT NULL DEFAULT 1,
    created_at TIMESTAMP NOT NULL,
    updated_at TIMESTAMP NOT NULL
);

-- Job run history
CREATE TABLE runs (
    id TEXT PRIMARY KEY,
    schedule_id TEXT,
    niche TEXT NOT NULL,
    status TEXT NOT NULL, -- 'running', 'completed', 'failed'
    started_at TIMESTAMP NOT NULL,
    completed_at TIMESTAMP,
    error_message TEXT,
    FOREIGN KEY (schedule_id) REFERENCES schedules(id)
);

-- Analysis results
CREATE TABLE opportunities (
    id TEXT PRIMARY KEY,
    run_id TEXT NOT NULL,
    niche TEXT NOT NULL,
    name TEXT NOT NULL,
    demand_score REAL NOT NULL,
    supply_score REAL NOT NULL,
    gap_score REAL NOT NULL,
    trend TEXT,
    data JSONB NOT NULL, -- Full opportunity data
    created_at TIMESTAMP NOT NULL,
    FOREIGN KEY (run_id) REFERENCES runs(id)
);
```

---

### API Endpoints

| Method | Endpoint | Description |
|--------|----------|-------------|
| GET | `/api/niches` | List available niches |
| POST | `/api/analyze` | Run one-off analysis |
| GET | `/api/opportunities` | Get opportunities (filtered by run_id, niche) |
| GET | `/api/schedules` | List all scheduled jobs |
| POST | `/api/schedules` | Create new schedule |
| PUT | `/api/schedules/:id` | Update schedule |
| DELETE | `/api/schedules/:id` | Delete schedule |
| GET | `/api/runs` | Get job run history |
| GET | `/api/runs/:id` | Get specific run details |
| GET | `/api/reports/:id` | Download report (Markdown/JSON) |

---

## Project Setup

### Repository Structure (Lean)

```
labs-nichefinder/
‚îú‚îÄ‚îÄ Cargo.toml                          # Workspace root
‚îú‚îÄ‚îÄ README.md                           # Project overview
‚îú‚îÄ‚îÄ PRD.md                              # This document
‚îú‚îÄ‚îÄ .gitmodules                         # Git submodules
‚îÇ
‚îú‚îÄ‚îÄ deps/
‚îÇ   ‚îî‚îÄ‚îÄ UDM-single/                     # Git submodule (use as-is)
‚îÇ
‚îú‚îÄ‚îÄ crates/
‚îÇ   ‚îú‚îÄ‚îÄ nichefinder-core/               # üì¶ Library: scoring, types, reporting
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ Cargo.toml
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ src/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ lib.rs
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ types.rs                # IntegrationOpportunity, NicheConfig
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ scoring.rs              # Demand-supply gap algorithm
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ reporting.rs            # Markdown/JSON formatters
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ nichefinder-server/             # üì¶ Binary: API + scheduler
‚îÇ       ‚îú‚îÄ‚îÄ Cargo.toml
‚îÇ       ‚îî‚îÄ‚îÄ src/
‚îÇ           ‚îú‚îÄ‚îÄ main.rs                 # Axum server entry point
‚îÇ           ‚îú‚îÄ‚îÄ api.rs                  # REST endpoints
‚îÇ           ‚îú‚îÄ‚îÄ scheduler.rs            # tokio-cron-scheduler
‚îÇ           ‚îî‚îÄ‚îÄ db.rs                   # SQLite with SQLx
‚îÇ
‚îú‚îÄ‚îÄ web-ui/                             # üì¶ React + TypeScript frontend
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ App.tsx                     # Single page app (no router)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ api.ts                      # Simple fetch wrapper
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ components/
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ NicheSelector.tsx
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ ResultsTable.tsx
‚îÇ   ‚îÇ       ‚îú‚îÄ‚îÄ OpportunityDetail.tsx
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ ScheduleManager.tsx
‚îÇ   ‚îú‚îÄ‚îÄ index.html
‚îÇ   ‚îú‚îÄ‚îÄ package.json
‚îÇ   ‚îú‚îÄ‚îÄ tsconfig.json
‚îÇ   ‚îú‚îÄ‚îÄ tailwind.config.js
‚îÇ   ‚îî‚îÄ‚îÄ vite.config.ts
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îî‚îÄ‚îÄ generate-connectors.sh          # One-time connector generation
‚îÇ
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îî‚îÄ‚îÄ niches/
‚îÇ       ‚îî‚îÄ‚îÄ home-assistant.yaml         # HA integration config (MVP)
‚îÇ
‚îî‚îÄ‚îÄ workflows/
    ‚îî‚îÄ‚îÄ home-assistant-analysis.yaml    # PEG workflow for HA
```

**Key Simplifications:**
- **2 crates only** (nichefinder-core + nichefinder-server)
- **No CLI crate** - use REST API or curl
- **No examples directory** - the server IS the example
- **Single niche config** - Home Assistant only for MVP
- **Flat component structure** - no pages/, api/ subdirectories

### Dependency Management

**Git Submodules:**
```bash
# Add UDM-Single as submodule
git submodule add https://github.com/Ngentix/UDM-single.git deps/UDM-single

# Initialize submodules
git submodule update --init --recursive
```

**Cargo Workspace:**
```toml
# Cargo.toml
[workspace]
members = [
    "crates/nichefinder-core",
    "crates/nichefinder-server",
]

[workspace.dependencies]
# ‚úÖ Reference UDM crates from submodule (use as-is, no modifications)
udm-peg = { path = "deps/UDM-single/crates/udm-peg" }
udm-connectors = { path = "deps/UDM-single/crates/udm-connectors" }

# Our crates
nichefinder-core = { path = "crates/nichefinder-core" }

# Server dependencies
axum = "0.7"
tower-http = { version = "0.5", features = ["cors"] }
sqlx = { version = "0.7", features = ["runtime-tokio", "sqlite"] }
tokio-cron-scheduler = "0.10"

# Common dependencies
tokio = { version = "1.0", features = ["full"] }
serde = { version = "1.0", features = ["derive"] }
serde_json = "1.0"
anyhow = "1.0"
tracing = "0.1"
tracing-subscriber = "0.3"
uuid = { version = "1.0", features = ["v4", "serde"] }
chrono = { version = "0.4", features = ["serde"] }
```

### Example Usage

#### Web UI (Primary Interface)

```bash
# Start the API server (includes scheduler)
cargo run --bin nichefinder-server

# In another terminal, start the React frontend
cd web-ui
npm run dev

# Open browser to http://localhost:5173 (Vite default)
# - Select "Home Assistant" from niche dropdown
# - Click "Run Analysis" for one-off analysis
# - Or create a schedule (e.g., "Daily at 9am")
# - View results in sortable table
# - Click opportunity for detailed view
```

#### REST API (Scripting / curl)

```bash
# Run one-off analysis
curl -X POST http://localhost:3001/api/analyze \
  -H "Content-Type: application/json" \
  -d '{"niche": "home-assistant"}'

# Get opportunities from latest run
curl http://localhost:3001/api/opportunities

# Create a daily schedule
curl -X POST http://localhost:3001/api/schedules \
  -H "Content-Type: application/json" \
  -d '{"niche": "home-assistant", "cron": "0 9 * * *"}'

# Export as JSON
curl http://localhost:3001/api/reports/latest > report.json
```

**Sample Output:**
```
üè† Home Assistant Integration Opportunities
============================================
Analysis Date: 2025-12-11
Data Sources: GitHub (47 issues), Reddit (234 posts), HACS (156 integrations)

Top 5 Opportunities:

1. Govee LED Advanced Control (Score: 94/100) üî•
   ‚îú‚îÄ Demand: 47 GitHub issues, 234 Reddit posts, 12 forum threads
   ‚îú‚îÄ Supply: 1 basic official integration, 2 unmaintained HACS integrations
   ‚îú‚îÄ Gap: Music sync, custom effects, advanced scheduling
   ‚îú‚îÄ Market Size: ~15,000 users
   ‚îú‚îÄ Trend: +45% growth (last 30 days)
   ‚îú‚îÄ Revenue Potential: $75K-225K
   ‚îî‚îÄ Sources: [GitHub #12345] [Reddit r/ha/abc123] [Forum thread/456]

2. Ecobee Advanced Scheduling (Score: 89/100)
   ...
```

---

## Implementation Plan

### Week 1: Project Setup & Architecture ‚úÖ

**Days 1-2: Repository Setup** ‚úÖ
- [x] Create GitHub repository: `labs-nichefinder`
- [x] Set up initial Cargo workspace structure
- [x] Add UDM-Single as git submodule
- [x] Fix UDM-Single compilation issues
- [x] Create PRD and architecture documents

**Days 3-5: Architecture Review & Lean Refactor** ‚úÖ
- [x] Review UDM-Single capabilities thoroughly
- [x] Identify what UDM-Single provides vs what we build
- [x] Simplify architecture: 6 crates ‚Üí 2 crates
- [x] Remove unnecessary components (CLI, separate scheduler)
- [x] Define minimal frontend (no React Query, Router, charts)
- [x] Update PRD with lean architecture

---

### Week 2: Core Library + Server + UI

**Days 1-2: nichefinder-core (Rust Library)**
- [ ] Create `nichefinder-core` crate structure
- [ ] Define `IntegrationOpportunity` type in `types.rs`
- [ ] Define `NicheConfig` type (from YAML)
- [ ] Implement scoring algorithm in `scoring.rs` (demand-supply gap)
- [ ] Implement JSON/Markdown formatters in `reporting.rs`
- [ ] Write unit tests for scoring logic

**Days 3-4: nichefinder-server (Rust Binary)**
- [ ] Create `nichefinder-server` crate structure
- [ ] Set up Axum with basic routes
- [ ] Set up SQLite with SQLx (schedules, runs, opportunities tables)
- [ ] Implement `/api/niches` endpoint
- [ ] Implement `/api/analyze` endpoint (triggers PEG workflow)
- [ ] Implement `/api/opportunities` endpoint
- [ ] Add CORS for local development

**Day 5: Scheduling + PEG Integration**
- [ ] Integrate tokio-cron-scheduler into server
- [ ] Implement `/api/schedules` CRUD endpoints
- [ ] Implement `/api/runs` endpoint (job history)
- [ ] Create PEG workflow YAML for Home Assistant
- [ ] Connect scheduler to PEG workflow executor
- [ ] Test: create schedule ‚Üí wait for trigger ‚Üí verify results stored

---

### Week 3: Frontend + Polish

**Days 1-2: React Frontend**
- [ ] Initialize React + TypeScript + Vite project
- [ ] Set up Tailwind CSS
- [ ] Create simple `api.ts` fetch wrapper
- [ ] Implement `NicheSelector` component
- [ ] Implement `ResultsTable` component (sortable)
- [ ] Implement `OpportunityDetail` component (inline expand)

**Days 3-4: Scheduling UI + Integration**
- [ ] Implement `ScheduleManager` component
- [ ] Implement `RunHistory` component
- [ ] Add "Run Now" button (one-off analysis)
- [ ] Add loading states and error handling
- [ ] Add JSON export button
- [ ] End-to-end testing: UI ‚Üí API ‚Üí PEG ‚Üí SQLite ‚Üí UI

**Day 5: Documentation + Demo**
- [ ] Write README with setup instructions
- [ ] Document the architecture (lean, 2 crates)
- [ ] Create demo walkthrough
- [ ] Record demo video (optional)

---

### Optional Week 4: Rate Limiting Contribution

**If time permits, contribute rate limiting to UDM-Single:**
- [ ] Fork UDM-Single or work in deps/UDM-single
- [ ] Add `RateLimitTracker` to `udm-connectors`
- [ ] Parse `X-RateLimit-*` headers from responses
- [ ] Implement backoff when limits approached
- [ ] Write tests
- [ ] Create PR to UDM-Single repository

---

### Future Enhancements (Post-MVP)

**Phase 2: AI Integration**
- [ ] Add OpenAI/Claude API connector
- [ ] Implement hybrid analysis (quantitative + AI insights)
- [ ] Add natural language query interface
- [ ] Example: "What Home Assistant integrations should I build for smart lighting?"
- [ ] AI generates analysis plan ‚Üí UDM/PEG executes ‚Üí AI synthesizes results

**Phase 3: Additional Niches**
- [ ] Developer Tools niche (GitHub + Stack Overflow + npm)
- [ ] SaaS Integration Marketplace niche (Zapier, Make, n8n)
- [ ] Content Creator niche (Reddit + News API + Hacker News)
- [ ] Custom niche builder (user-defined configurations)

**Phase 4: Advanced Features**
- [ ] Web UI for non-technical users
- [ ] Automated monitoring and alerts
- [ ] Historical trend tracking and visualization
- [ ] Collaborative opportunity scoring
- [ ] Integration with project management tools

**Phase 5: Natural Language Interface**
- [ ] Conversational query interface
- [ ] "Show me trending Home Assistant integration requests from this week"
- [ ] "Compare Govee vs Philips Hue integration opportunities"
- [ ] "Alert me when a new high-scoring opportunity emerges"
- [ ] AI-powered query understanding and workflow generation

---

## Success Criteria

### Technical Validation

| Criterion | Target | Status |
|-----------|--------|--------|
| **Connector Generation** | < 5 minutes | üîÑ In Progress |
| **Multi-Source Data Collection** | 3+ APIs per niche | üìã Planned |
| **Workflow Execution** | > 95% success rate | üìã Planned |
| **Performance** | < 2 minutes per analysis | üìã Planned |
| **Data Freshness** | Real-time (< 1 hour old) | üìã Planned |

### Business Validation

| Criterion | Target | Status |
|-----------|--------|--------|
| **Cost** | $0 (free tier APIs only) | ‚úÖ Achieved |
| **Accuracy** | > 70% opportunity identification | üìã To Validate |
| **Verifiability** | 100% sources linked | üìã Planned |
| **Repeatability** | Consistent scores ¬±5% | üìã To Validate |

### Value Proposition Validation

| Criterion | Target |
|-----------|--------|
| **vs ChatGPT: Data Freshness** | Real-time vs months-old training data |
| **vs ChatGPT: Verifiability** | 100% sources linked vs unverifiable |
| **vs ChatGPT: Quantification** | Precise scores vs qualitative estimates |
| **vs ChatGPT: Automation** | Scheduled analysis vs manual queries |
| **Complementarity** | Can feed structured data to AI for enhanced insights |

---

## Key Differentiators

### 1. Real-Time Market Intelligence
Unlike AI tools with training cutoffs, NicheFinder provides current data from live APIs, ensuring opportunities are based on the latest market signals.

### 2. Verifiable, Quantitative Analysis
Every opportunity includes precise metrics and source links, enabling validation and trust in the analysis.

### 3. Systematic, Repeatable Process
Consistent methodology ensures comparable results over time, enabling trend tracking and automated monitoring.

### 4. Multi-Source Data Fusion
UDM normalizes data from heterogeneous sources (GitHub, Reddit, forums, etc.) into unified schemas, providing comprehensive market views.

### 5. Extensible Architecture
Multi-niche design allows analysis of any integration ecosystem, demonstrating UDM/PEG versatility.

### 6. AI Complementarity
Structured output can feed AI tools for enhanced qualitative insights, combining quantitative rigor with contextual reasoning.

---

## Risks & Mitigations

| Risk | Impact | Mitigation |
|------|--------|------------|
| **API Rate Limits** | High | Use free tier APIs with generous limits; implement caching |
| **Data Quality** | Medium | Validate against known opportunities; implement confidence scores |
| **Scope Creep** | Medium | Focus on Home Assistant first; add niches incrementally |
| **AI Comparison** | Low | Position as complementary, not competitive; emphasize unique value |
| **Maintenance Burden** | Medium | Use stable APIs; implement graceful degradation |

---

## Next Steps

### Immediate (Week 1, Day 5)
1. ‚úÖ Update PRD with Home Assistant use case
2. [ ] Generate GitHub API connector
3. [ ] Generate Reddit API connector
4. [ ] Test connector generation end-to-end
5. [ ] Document setup process

### Short-Term (Week 2)
1. [ ] Define IntegrationOpportunity schema
2. [ ] Implement UDM mappings
3. [ ] Create PEG workflow for Home Assistant analysis
4. [ ] Test multi-source data collection

### Medium-Term (Week 3)
1. [ ] Implement scoring algorithm
2. [ ] Build CLI with niche selector
3. [ ] Generate sample reports
4. [ ] Create demo materials

### Long-Term (Post-MVP)
1. [ ] Add AI integration for hybrid analysis
2. [ ] Implement natural language interface
3. [ ] Add additional niches (dev tools, SaaS, etc.)
4. [ ] Build web UI

---

**Version History:**
- v1.0 (2025-12-11): Initial PRD with local business niche use case
- v2.0 (2025-12-11): Pivot to Home Assistant integrations, multi-niche architecture, AI complementarity

---

**For implementation details and progress tracking, see CONVERSATION_HANDOFF.md**
